{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MapReduce\n",
    "Reduce operations **must not depend on the order of application**\n",
    "\n",
    "# Execution Plan\n",
    "**Lazy Execution** refers to this type of behaviour. The system delays actual computation until the latest possible moment. Instead of computing the content of the RDD, it adds the RDD to the **execution plan**.\n",
    "\n",
    "Using Lazy evaluation of a plan has two main advantages relative to immediate execution of each step:\n",
    "1. A single pass over the data, rather than multiple passes.\n",
    "2. Smaller memory footprint becase no intermediate results are saved.\n",
    "\n",
    "**cache** `.cache()` to cache intermediate results in memory so that we can reuse them without recalculating them. Notice that cache is also **lazy cache** - not cached until the latest possible moment.\n",
    "\n",
    "\n",
    "# Key Value\n",
    "**count occurrence**\n",
    "```py\n",
    "rdd.map(lambda t: (t, 1)).reduce(add)\n",
    "```\n",
    "\n",
    "**sortByKey**. RDDs **do** have a meaningful order, which extends between partitions.\n",
    "\n",
    "**sortBy(keyfunc)**.\n",
    "\n",
    "**groupByKey()**. Returns a new RDD of `(key,<iterator>)` rather than materialized list\n",
    "\n",
    "**avg**\n",
    "```py\n",
    "(rdd\n",
    "   .mapValues(lambda x: np.array([float(x[0]), 1]))  # (value, 1)\n",
    "   .reduceByKey(add)\n",
    "   .mapValues(lambda x: float(x[0])/x[1])\n",
    "   .collect()\n",
    ")\n",
    "```\n",
    "\n",
    "# Paritition\n",
    "**glom**\n",
    "```py\n",
    "A.partitionBy(3).glom().collect()\n",
    "```\n",
    "\n",
    "**Broadcasting**. `sc.broadcast(...)`. Without broadcasting: \"Normally, when a function passed to a Spark operation (such as map or reduce) is executed on a remote cluster node, it works on separate copies of all the variables used in the function. These variables are copied to each machine, and no updates to the variables on the remote machine are propagated back to the driver program. Supporting general, read-write shared variables across tasks would be inefficient. \"\n",
    " \n",
    "With broadcasting: \"Broadcast variables allow the programmer to keep a read-only variable cached on each machine rather than shipping a copy of it with tasks.\"\n",
    "\n",
    "# SQL\n",
    "**Why schema**. Schema as structured data in lieu of nested tuple. Turning unstructured data to structured data.\n",
    "```py\n",
    "sqlContext = SQLContext(sc)\n",
    "from pyspark.sql.types import Row, StructField, StructType, StringType, IntegerType, FloatType\n",
    "schema = StructType([StructField(\"word\", StringType(), False),\n",
    "                     StructField(\"count\", IntegerType(), False),\n",
    "                     StructField(\"prob\", FloatType(), False)])\n",
    "\n",
    "unigram = sqlContext.createDataFrame(\n",
    "        freq_ngrams[1].map(lambda (cnt, v): (v[0], cnt, float(cnt/total_cnt))), schema\n",
    ")\n",
    "unigram.take(5)\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
