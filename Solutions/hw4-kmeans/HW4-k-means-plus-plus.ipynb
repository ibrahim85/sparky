{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means++\n",
    "\n",
    "In this notebook, we are going to implement [k-means++](https://en.wikipedia.org/wiki/K-means%2B%2B) algorithm with multiple initial sets. The original k-means++ algorithm will just sample one set of initial centroid points and iterate until the result converges. The only difference in this implementation is that we will sample `RUNS` sets of initial centroid points and update them in parallel. The procedure will finish when all centroid sets are converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Name: Danyang Zhang\n",
    "# Email: daz040@eng.ucsd.edu\n",
    "# PID: A53104006\n",
    "PYBOLT = False\n",
    "if PYBOLT:\n",
    "    from pyspark import SparkContext\n",
    "    sc = SparkContext()\n",
    "\n",
    "### Definition of some global parameters.\n",
    "K = 5  # Number of centroids\n",
    "RUNS = 25  # Number of K-means runs that are executed in parallel. Equivalently, number of sets of initial points\n",
    "RANDOM_SEED = 60295531\n",
    "converge_dist = 0.1 # The K-means algorithm is terminated when the change in the location \n",
    "                    # of the centroids is smaller than 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "from numpy.linalg import norm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def print_log(s):\n",
    "    sys.stdout.write(s + \"\\n\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "def parse_data(row):\n",
    "    '''\n",
    "    Parse each pandas row into a tuple of (station_name, feature_vec),\n",
    "    where feature_vec is the concatenation of the projection vectors\n",
    "    of TAVG, TRANGE, and SNWD.\n",
    "    '''\n",
    "    return (row[0],  # station name \n",
    "            np.concatenate([row[1], row[2], row[3]])  # TAVG, TRANGE, and SNWD\n",
    "           ) \n",
    "\n",
    "\n",
    "def compute_entropy(d):\n",
    "    '''\n",
    "    Compute the entropy given the frequency vector `d`\n",
    "    Vectorized \n",
    "    '''\n",
    "    d = np.array(d)\n",
    "    d = 1.0 * d / d.sum()\n",
    "    return -np.sum(d * np.log2(d))\n",
    "\n",
    "\n",
    "def choice(p):\n",
    "    '''\n",
    "    Generates a random sample from [0, len(p)),\n",
    "    where p[i] is the probability associated with i. \n",
    "    '''\n",
    "    random = np.random.random()\n",
    "    r = 0.0\n",
    "    for idx in range(len(p)):\n",
    "        r = r + p[idx]\n",
    "        if r > random:\n",
    "            return idx\n",
    "    assert(False)\n",
    "\n",
    "\n",
    "def kmeans_init(rdd, K, RUNS, seed):\n",
    "    '''\n",
    "    Select `RUNS` sets of initial points for `K`-means++\n",
    "    '''\n",
    "    # the `centers` variable is what we want to return\n",
    "    n_data = rdd.count()  # T\n",
    "    shape = rdd.take(1)[0][1].shape[0]  # from parse_data, shape=dim=9\n",
    "    centers = np.zeros((RUNS, K, shape))\n",
    "\n",
    "    def update_dist(vec, dist, k):\n",
    "        new_dist = norm(vec - centers[:, k], axis=1)**2  # col vector \n",
    "        return np.min([dist, new_dist], axis=0)\n",
    "\n",
    "\n",
    "    # The second element `dist` in the tuple below is the closest distance from\n",
    "    # each data point to the selected points in the initial set, where `dist[i]`\n",
    "    # is the closest distance to the points in the i-th initial set (RUNS).\n",
    "\n",
    "    # data point -> dist\n",
    "    data = rdd.map(lambda p: (p, [np.inf]*RUNS)).cache()  \n",
    "    \n",
    "    # Collect the feature vectors of all data points beforehand, might be\n",
    "    # useful in the following for-loop\n",
    "    local_data = rdd.map(lambda (name, vec): vec).collect()\n",
    "    local_data = np.array(local_data)\n",
    "    # Randomly select the FIRST center point for every run of k-means++,\n",
    "    # i.e. randomly select `RUNS` points and add it to the `centers` variable\n",
    "    sample = [local_data[run] for run in np.random.randint(0, len(local_data), RUNS)]\n",
    "    centers[:, 0] = sample  # shape: (RUNS, K, shape), the first \n",
    "    \n",
    "    \n",
    "    # after selecting the first centroid, select the remaining \n",
    "    for idx in range(K - 1):\n",
    "        ##############################################################################\n",
    "        # Insert your code here:\n",
    "        ##############################################################################\n",
    "        # In each iteration, you need to select one point for each set\n",
    "        # of initial points (so select `RUNS` points in total).\n",
    "        # For each data point x, let D_i(x) be the distance between x and\n",
    "        # the nearest center that has already been added to the i-th set.\n",
    "        # Choose a new data point for i-th set using a weighted probability\n",
    "        # where point x is chosen with probability proportional to D_i(x)^2\n",
    "        ##############################################################################      \n",
    "        c = idx\n",
    "        c_new = idx + 1  # choosing the (i+1)-th centroid \n",
    "        \n",
    "        ## Numpy Vectorization\n",
    "        dp = np.repeat(local_data[np.newaxis, :], RUNS, axis=0)\n",
    "        dist = np.zeros((RUNS, n_data), dtype=np.float64) + np.inf\n",
    "        for c in xrange(c_new):  # No Need, consider loop invariant of min_dist\n",
    "            new_dist = norm(dp - np.repeat(centers[:, c][:, np.newaxis], n_data, axis=1), axis=2)**2  # col vector \n",
    "            dist = np.minimum(dist, new_dist)\n",
    "\n",
    "        ## Naive \n",
    "        #for t in xrange(n_data):\n",
    "        #    dist[:, t] = update_dist(dp[:, t], dist[:, t], c)\n",
    "\n",
    "        ## Spark\n",
    "        #data = data.map(lambda (p, dist): (p, update_dist(p[1], dist, c)))  # p[1] is the vec\n",
    "        #dist = np.array(data.values().collect()).T\n",
    "        \n",
    "        assert dist.shape == (RUNS, n_data)\n",
    "        \n",
    "        # normalized to calculate probability, with another dimension RUNS\n",
    "        dist_sum = np.sum(dist, axis=1)\n",
    "        prob = np.divide(dist, np.repeat(dist_sum[:, np.newaxis], n_data, axis=1))\n",
    "        \n",
    "        ## Naive\n",
    "        #for r in xrange(RUNS):\n",
    "        #    i = choice(P[r])            \n",
    "        #    centers[r, c_new] = local_data[i]\n",
    "        \n",
    "        ## Vectorize for axis of RUNS\n",
    "        i = np.apply_along_axis(choice, 1, prob)            \n",
    "        centers[:, c_new] = local_data[i]  # dp: (RUNS, T, dim)\n",
    "\n",
    "    return centers\n",
    "\n",
    "\n",
    "# why not vectorize - using spark MapReduce\n",
    "def get_closest(p, centers):\n",
    "    '''\n",
    "    Get closes for each run \n",
    "    Return the indices the nearest centroids of `p`.\n",
    "    `centers` contains sets of centroids, where `centers[r]` is\n",
    "    the r-th set of centroids. (RUNS)\n",
    "    '''\n",
    "    best = [0] * len(centers)\n",
    "    closest = [np.inf] * len(centers)\n",
    "    for r in range(len(centers)):\n",
    "        for k in range(len(centers[0])):\n",
    "            temp_dist = norm(p - centers[r][k])\n",
    "            if temp_dist < closest[r]:\n",
    "                closest[r] = temp_dist\n",
    "                best[r] = k\n",
    "    return best\n",
    "\n",
    "\n",
    "def kmeans(rdd, K, RUNS, converge_dist, seed):\n",
    "    '''\n",
    "    Run K-means++ algorithm on `rdd`, where `RUNS` is the number of\n",
    "    initial sets to use.\n",
    "    '''\n",
    "    k_points = kmeans_init(rdd, K, RUNS, seed)\n",
    "    print_log(\"Initialized.\")\n",
    "    temp_dist = 1.0\n",
    "\n",
    "    iters = 0\n",
    "    st = time.time()\n",
    "    \n",
    "    n_data = rdd.count()  # T\n",
    "    shape = rdd.take(1)[0][1].shape[0]  # from parse_data, shape=dim=3\n",
    "    local_data_rdd = rdd.map(lambda (name, vec): vec).cache()\n",
    "    local_data = np.array(local_data_rdd.collect())\n",
    "    \n",
    "    while temp_dist > converge_dist:\n",
    "        ##############################################################################\n",
    "        # INSERT YOUR CODE HERE\n",
    "        ##############################################################################\n",
    "        \n",
    "        # Update all `RUNS` sets of centroids using standard k-means algorithm\n",
    "        # Outline:\n",
    "        #   - For each point x, select its nearest centroid in i-th centroids set\n",
    "        #   - Average all points that are assigned to the same centroid\n",
    "        #   - Update the centroid with the average of all points that are assigned to it\n",
    "        \n",
    "        # Insert your code here\n",
    "        new_points = {}\n",
    "        \n",
    "        ## Numpy vectorization\n",
    "        # C = np.apply_along_axis(lambda x: get_closest(x, k_points), 1, local_data)        \n",
    "        \n",
    "        ## Naive\n",
    "        #for t in xrange(n_data):\n",
    "        #    c = get_closest(local_data[t], k_points)\n",
    "        #    for r, k in enumerate(c):\n",
    "        #        cluster[(r, k)].append(local_data[t])\n",
    "        \n",
    "        ## Spark \n",
    "        C = local_data_rdd.map(lambda x: get_closest(x, k_points))\n",
    "        C = np.array(C.collect())\n",
    "        assert C.shape == (n_data, RUNS)  # (t, r) -> cluster label\n",
    "        for r in xrange(RUNS):\n",
    "            for k in xrange(K):\n",
    "                cluster_dp = local_data[C[:, r]==k]  # cluster data point\n",
    "                new_points[(r, k)] = cluster_dp.mean(axis=0)\n",
    "                ## otherwise, spark reduce is slower than collect for numpy due to overhead of reduce\n",
    "    \n",
    "        # You can modify this statement as long as `temp_dist` equals to\n",
    "        # max( sum( l2_norm of the movement of j-th centroid in each centroids set ))\n",
    "        ##############################################################################\n",
    "        temp_dist = np.max([\n",
    "                np.sum([norm(k_points[r, k] - new_points[(r, k)]) for k in range(K)])\n",
    "                for r in range(RUNS)\n",
    "        ])   # move distance, max over all runs & all points \n",
    "\n",
    "        iters = iters + 1\n",
    "        if iters % 5 == 0:\n",
    "            print_log(\"Iteration %d max shift: %.2f (time: %.2f)\" %\n",
    "                      (iters, temp_dist, time.time() - st))\n",
    "            st = time.time()\n",
    "\n",
    "        # update old centroids\n",
    "        # You modify this for-loop to meet your need\n",
    "        for ((r, k), p) in new_points.items():  ## pattern matching in python dict\n",
    "            k_points[r, k] = p\n",
    "\n",
    "    return k_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'USC00044534', array([  3.04796236e+03,   1.97434852e+03,   1.50560792e+02,\n",
       "          -2.90363288e+03,  -2.36907268e+02,   1.47021791e+02,\n",
       "           1.91503001e-01,   1.87262808e-01,  -4.01379553e-02]))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Read data\n",
    "if PYBOLT:\n",
    "    path = \"../Data/Weather/stations_projections.pickle\"\n",
    "else:\n",
    "    path = \"../../Data/Weather/stations_projections.pickle\"\n",
    "    \n",
    "data = pickle.load(open(path, \"rb\"))\n",
    "rdd = sc.parallelize([parse_data(row[1]) for row in data.iterrows()])\n",
    "rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized.\n",
      "Iteration 5 max shift: 3211.03 (time: 24.43)\n",
      "Iteration 10 max shift: 1928.05 (time: 20.41)\n",
      "Iteration 15 max shift: 693.41 (time: 18.94)\n",
      "Iteration 20 max shift: 348.29 (time: 17.63)\n",
      "Iteration 25 max shift: 235.29 (time: 20.81)\n",
      "Iteration 30 max shift: 185.35 (time: 21.44)\n",
      "Iteration 35 max shift: 51.71 (time: 23.75)\n",
      "Iteration 40 max shift: 45.07 (time: 23.32)\n",
      "Iteration 45 max shift: 26.03 (time: 19.06)\n",
      "Iteration 50 max shift: 15.59 (time: 18.09)\n",
      "Iteration 55 max shift: 0.85 (time: 18.69)\n",
      "Time takes to converge: 235.439112902\n"
     ]
    }
   ],
   "source": [
    "# main code\n",
    "\n",
    "import time\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "centroids = kmeans(rdd, K, RUNS, converge_dist, np.random.randint(1000))\n",
    "group = rdd.mapValues(lambda p: get_closest(p, centroids)).collect()\n",
    "\n",
    "print \"Time takes to converge:\", time.time() - st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify your results\n",
    "Verify your results by computing the objective function of the k-means clustering problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_cost(rdd, centers):\n",
    "    '''\n",
    "    Compute the square of l2 norm from each data point in `rdd`\n",
    "    to the centroids in `centers`\n",
    "    '''\n",
    "    def _get_cost(p, centers):\n",
    "        best = [0] * len(centers)\n",
    "        closest = [np.inf] * len(centers)\n",
    "        for idx in range(len(centers)):\n",
    "            for j in range(len(centers[0])):\n",
    "                temp_dist = norm(p - centers[idx][j])\n",
    "                if temp_dist < closest[idx]:\n",
    "                    closest[idx] = temp_dist\n",
    "                    best[idx] = j\n",
    "        return np.array(closest)**2\n",
    "    \n",
    "    cost = rdd.map(lambda (name, v): _get_cost(v, centroids)).collect()\n",
    "    return np.array(cost).sum(axis=0)\n",
    "\n",
    "cost = get_cost(rdd, centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.8254902123 33.7575332525 33.7790236109\n"
     ]
    }
   ],
   "source": [
    "log2 = np.log2\n",
    "\n",
    "print log2(np.max(cost)), log2(np.min(cost)), log2(np.mean(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the increase of entropy after multiple runs of k-means++\n",
    "\n",
    "$$\n",
    "H(G^{(1)}, G^{(2)}) = H(D) \n",
    "$$\n",
    "\n",
    "For $i$-th element in $D$, $D_i$ is for $i$-th data point $X^{(i)}$, with the probability/frequency of tuple $(r^{(1)}_i, r^{(2)}_i)$, where $r^{(t)}_i$ is the centroid of the cluster containing $X^{(i)}$ in clustering run $G^{(t)}$.\n",
    "\n",
    "Test the consistency of different clustering runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'USC00044534', [0, 3, 4, 2, 0, 0, 1, 2, 4, 1, 4, 0, 3, 0, 3, 0, 0, 1, 0, 0, 3, 0, 1, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "entropy = []\n",
    "\n",
    "print group[0]\n",
    "\n",
    "for r in range(RUNS):\n",
    "    count = defaultdict(int)\n",
    "    for dp, cluster in group:  # cluster hereby represented by its center point \n",
    "        cluster_vec = ','.join(map(str, cluster[:r+1]))\n",
    "        count[cluster_vec] += 1\n",
    "        \n",
    "    entropy.append(compute_entropy(count.values()))  # frequency vector "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Remove this cell before submitting to PyBolt (PyBolt does not fully support matplotlib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEPCAYAAABCyrPIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc73c02b910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not PYBOLT:\n",
    "    %matplotlib inline\n",
    "\n",
    "    plt.xlabel(\"Runs\")\n",
    "    plt.ylabel(\"Entropy\")\n",
    "    plt.plot(range(1, RUNS + 1), entropy)\n",
    "    2**entropy[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "F(i) = H(G_1, G_2, ..., G_i) \n",
    "$$\n",
    "\n",
    "$F(i)$ stablizes multiple runs, thus the clustering is stable "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entropy= [1.6445469704935676, 2.0800064512748428, 2.080006451274842, 2.0800064512748424, 2.1906681946052755, 2.2570115065383876, 2.2786597860645408, 2.2786597860645408, 2.2786597860645408, 2.2786597860645408, 2.2786597860645403, 2.2786597860645408, 2.2786597860645408, 2.2786597860645408, 2.2849509629282276, 2.2849509629282276, 2.2849509629282276, 2.2849509629282272, 2.286874405497795, 2.2868744054977945, 2.2868744054977945, 2.286874405497795, 2.2868744054977945, 2.286874405497795, 2.286874405497795]\n",
      "best_centers= [array([ 2952.76608   ,  1933.02980077,    92.424188  , -2547.74851278,\n",
      "         144.84123959,   154.0172669 ,    18.40817384,     7.84926361,\n",
      "           5.11113863]), array([  428.4738994 ,  1807.58033164,    35.14799298, -2574.43476306,\n",
      "        -180.39839191,   263.09089521,  6048.90511888,  -743.20856056,\n",
      "         256.68319372]), array([ 1492.0570036 ,  1954.30230067,    94.48584365, -2567.99675086,\n",
      "        -112.2682711 ,   152.28015089,   395.84574671,   131.09390181,\n",
      "          73.10315542]), array([  750.10763916,  2067.97627806,    35.34601332, -2398.58742321,\n",
      "        -138.36631381,   233.32209536,  2268.85311051,   245.99611499,\n",
      "         125.46432194]), array([   408.29696084,   1353.92836359,     56.37619358,  -2206.17029272,\n",
      "         -221.37785013,    183.25193705,  18757.57406286,  -5513.4828535 ,\n",
      "         1476.58182765])]\n"
     ]
    }
   ],
   "source": [
    "print 'entropy=', entropy\n",
    "best = np.argmin(cost)\n",
    "print 'best_centers=', list(centroids[best])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
