{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Word Count\n",
    "Counting the number of occurances of words in a text is one of the most popular first eercises when learning Map-Reduce Programming. It is the equivalent to `Hello World!` in regular programming.\n",
    "\n",
    "We will do it two way, a simpler way where sorting is done after the RDD is collected, and a more sparky way, where the sorting is also done using an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[00m../Data/Moby-Dick.txt\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# First, check that the text file is where we expect it to be\n",
    "%ls ../Data/Moby-Dick.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the text file into an RDD\n",
    "Note that, as execution is Lazy, this does not necessarily mean that actual reading of the file content has occured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 805 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text_file = sc.textFile(u'../Data/Moby-Dick.txt')\n",
    "type(text_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the words\n",
    "Next, we count the number of words that occured in the text. Again, this is only setting the plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "CPU times: user 16 ms, sys: 12 ms, total: 28 ms\n",
      "Wall time: 53.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(add)\n",
    "print type(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Have a look a the execution plan\n",
    "Note that the earliest node in the dependency graph is the file `../../Data/Moby-Dick.txt`. It is possible that that even the first element in that file has not yet been read!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) PythonRDD[18] at RDD at PythonRDD.scala:43 []\n",
      " |  MapPartitionsRDD[17] at mapPartitions at PythonRDD.scala:342 []\n",
      " |  ShuffledRDD[16] at partitionBy at NativeMethodAccessorImpl.java:-2 []\n",
      " +-(2) PairwiseRDD[15] at reduceByKey at <timed exec>:1 []\n",
      "    |  PythonRDD[14] at reduceByKey at <timed exec>:1 []\n",
      "    |  MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:-2 []\n",
      "    |  ../Data/Moby-Dick.txt HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:-2 []\n"
     ]
    }
   ],
   "source": [
    "print counts.toDebugString()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count!\n",
    "Finally we count the number of times each word has occured.\n",
    "Note that this cell, finaally, the Lazy execution model finally performs some actual work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count=33782.000000, sum=219480.000000, mean=6.496951\n",
      "CPU times: user 4 ms, sys: 8 ms, total: 12 ms\n",
      "Wall time: 258 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "V_sz = counts.count()  \n",
    "tf_total = counts.map(lambda (w,i): i).reduce(lambda x,y:x+y)  \n",
    "print 'count=%f, sum=%f, mean=%f'%(V_sz, tf_total, float(tf_total)/V_sz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect the `Sum` RDD into the driver node\n",
    "This also takes significant work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n",
      "CPU times: user 12 ms, sys: 8 ms, total: 20 ms\n",
      "Wall time: 155 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "C = counts.collect()\n",
    "print type(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort \n",
    "Now that we have collected the Sum RDD into the driver node, we no longer rely on Spark. The following two cells\n",
    "are simple python commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most common words [(u'I', 1724), (u'his', 2415), (u'that', 2693), (u'in', 3878), (u'', 4347), (u'to', 4510), (u'a', 4533), (u'and', 5951), (u'of', 6587), (u'the', 13766)]\n",
      "Least common words [(u'funereal', 1), (u'unscientific', 1), (u'lime-stone,', 1), (u'shouted,', 1), (u'pitch-pot,', 1), (u'cod-liver', 1), (u'prices', 1), (u'prefix', 1), (u'boots.\"', 1), (u'slew.', 1)]\n"
     ]
    }
   ],
   "source": [
    "# sort using Python list \n",
    "C.sort(key=lambda x: x[1])\n",
    "print 'most common words',C[-10:]\n",
    "print 'Least common words',C[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the mean number of occurances per word.\n",
    "\n",
    "Without rely on spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count2=33782.000000, sum2=219480.000000, mean2=6.496951\n"
     ]
    }
   ],
   "source": [
    "Count2 = len(C)\n",
    "Sum2 = sum(i for w,i in C)\n",
    "print 'count2=%f, sum2=%f, mean2=%f'%(Count2,Sum2,float(Sum2)/Count2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Count in Pure Spark\n",
    "We now show how to perform word count, including sorting, using RDDs, returning to the driver node just the top 10 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 66 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "RDD = text_file.flatMap(lambda x: x.split(' '))\\\n",
    "    .filter(lambda x: x!='')\\\n",
    "    .map(lambda word: (word,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16 ms, sys: 4 ms, total: 20 ms\n",
      "Wall time: 49.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "RDD1 = RDD.reduceByKey(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 31 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "RDD2 = RDD1.map(lambda (t, f): (f, t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sortByKey` takes significant amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 24 ms, total: 24 ms\n",
      "Wall time: 267 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "RDD3 = RDD2.sortByKey(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) PythonRDD[39] at RDD at PythonRDD.scala:43 []\n",
      " |  MapPartitionsRDD[38] at mapPartitions at PythonRDD.scala:342 []\n",
      " |  ShuffledRDD[37] at partitionBy at NativeMethodAccessorImpl.java:-2 []\n",
      " +-(2) PairwiseRDD[36] at sortByKey at <timed exec>:1 []\n",
      "    |  PythonRDD[35] at sortByKey at <timed exec>:1 []\n",
      "    |  MapPartitionsRDD[26] at mapPartitions at PythonRDD.scala:342 []\n",
      "    |  ShuffledRDD[25] at partitionBy at NativeMethodAccessorImpl.java:-2 []\n",
      "    +-(2) PairwiseRDD[24] at reduceByKey at <timed exec>:1 []\n",
      "       |  PythonRDD[23] at reduceByKey at <timed exec>:1 []\n",
      "       |  MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:-2 []\n",
      "       |  ../Data/Moby-Dick.txt HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:-2 []\n"
     ]
    }
   ],
   "source": [
    "print RDD3.toDebugString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 ms, sys: 0 ns, total: 8 ms\n",
      "Wall time: 210 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(13766, u'the'),\n",
       " (6587, u'of'),\n",
       " (5951, u'and'),\n",
       " (4533, u'a'),\n",
       " (4510, u'to'),\n",
       " (3878, u'in'),\n",
       " (2693, u'that'),\n",
       " (2415, u'his'),\n",
       " (1724, u'I'),\n",
       " (1692, u'with')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "RDD3.take(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
